#!/usr/bin/env python3

import sys
import os
import csv
import ffmpeg
from pyannote.audio import Pipeline

def main():
    if len(sys.argv) != 2:
        print("Usage: speaker-time <audio_file>")
        sys.exit(1)

    file_path = sys.argv[1]
    absolute_path = os.path.abspath(file_path)

    # Initialize pyannote pipeline
    hf_token = os.getenv("HUGGINGFACE_SPEAKER_DIARIZATION")
    if not hf_token:
        print("Error: HUGGINGFACE_SPEAKER_DIARIZATION environment variable not set")
        sys.exit(1)

    pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1", use_auth_token=hf_token)

    # Run speaker diarization
    diarization = pipeline(absolute_path)

    # Collect all segments for timeline processing
    segments = []
    for segment, _, speaker in diarization.itertracks(yield_label=True):
        segments.append({
            'speaker': speaker,
            'start': segment.start,
            'end': segment.end
        })

    # Sort segments by start time
    segments.sort(key=lambda x: x['start'])

    # Resolve overlapping segments (assign to original speaker)
    resolved_segments = []
    for segment in segments:
        # Check for overlaps with existing resolved segments
        overlapping = False
        for existing in resolved_segments:
            if (segment['start'] < existing['end'] and segment['end'] > existing['start']):
                # Overlap detected - keep the original speaker's segment
                overlapping = True
                break
        
        if not overlapping:
            resolved_segments.append(segment)

    # Generate output directory and CSV filename
    base_name = os.path.splitext(os.path.basename(file_path))[0]
    output_dir = os.path.join("output", base_name)
    os.makedirs(output_dir, exist_ok=True)
    csv_filename = os.path.join(output_dir, f"{base_name}_timeline.csv")

    # Write timeline to CSV
    with open(csv_filename, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['SPEAKER_ID', 'start_time', 'end_time'])
        
        for segment in resolved_segments:
            writer.writerow([segment['speaker'], segment['start'], segment['end']])

    print(f"Timeline saved to: {csv_filename}")
    print(f"Total segments: {len(resolved_segments)}")

    # Create audio subdirectory for segment files
    audio_dir = os.path.join(output_dir, "audio")
    os.makedirs(audio_dir, exist_ok=True)

    # Read CSV and cut audio segments
    print("Cutting audio segments...")
    with open(csv_filename, 'r') as csvfile:
        reader = csv.DictReader(csvfile)
        for i, row in enumerate(reader, 1):
            start_time = float(row['start_time'])
            end_time = float(row['end_time'])
            
            # Generate 6-digit zero-padded filename
            segment_filename = os.path.join(audio_dir, f"{i:06d}.mp3")
            
            # Cut audio segment using ffmpeg-python
            stream = ffmpeg.input(absolute_path, ss=start_time, to=end_time)
            ffmpeg.output(stream, segment_filename).run(overwrite_output=True, quiet=True)
    
    print(f"Audio segments saved to: {audio_dir}")
    print(f"Generated {len(resolved_segments)} audio files")

    # Generate language detection samples
    generate_language_samples(resolved_segments, absolute_path, output_dir)

def find_contiguous_segments(segments, gap_threshold=3.0):
    """Group segments by speaker and find contiguous periods within gap threshold."""
    speaker_groups = {}
    
    # Group segments by speaker
    for segment in segments:
        speaker = segment['speaker']
        if speaker not in speaker_groups:
            speaker_groups[speaker] = []
        speaker_groups[speaker].append(segment)
    
    # Find contiguous periods for each speaker
    contiguous_periods = {}
    for speaker, speaker_segments in speaker_groups.items():
        periods = []
        current_period = [speaker_segments[0]]
        
        for i in range(1, len(speaker_segments)):
            gap = speaker_segments[i]['start'] - current_period[-1]['end']
            
            if gap <= gap_threshold:
                # Continue current period
                current_period.append(speaker_segments[i])
            else:
                # Start new period
                periods.append(current_period)
                current_period = [speaker_segments[i]]
        
        # Add the last period
        periods.append(current_period)
        contiguous_periods[speaker] = periods
    
    return contiguous_periods

def generate_language_samples(segments, audio_file_path, output_dir):
    """Generate 30-60 second audio samples for language detection."""
    print("Generating language detection samples...")
    
    # Find contiguous segments
    contiguous_periods = find_contiguous_segments(segments)
    
    # Create language detection subdirectory
    lang_dir = os.path.join(output_dir, "language_detection")
    os.makedirs(lang_dir, exist_ok=True)
    
    for speaker, periods in contiguous_periods.items():
        # Calculate total speaking time for this speaker
        total_duration = sum(seg['end'] - seg['start'] for seg in segments if seg['speaker'] == speaker)
        
        # Skip speakers with less than 20 seconds total
        if total_duration < 20:
            print(f"Skipping {speaker}: only {total_duration:.1f}s total speaking time")
            continue
        
        # Find best combination of periods to get 20-60 seconds
        best_sample = find_best_sample_combination(periods, target_min=20, target_max=60)
        
        if best_sample:
            sample_filename = os.path.join(lang_dir, f"{speaker}_language_sample.mp3")
            create_combined_audio_sample(best_sample, audio_file_path, sample_filename)
            
            sample_duration = sum(seg['end'] - seg['start'] for seg in best_sample)
            print(f"Generated {speaker} language sample: {sample_duration:.1f}s -> {sample_filename}")
        else:
            print(f"Could not generate sufficient sample for {speaker}")

def find_best_sample_combination(periods, target_min=30, target_max=60):
    """Find the best combination of contiguous periods to reach target duration."""
    # Sort periods by total duration (largest first)
    period_durations = []
    for period in periods:
        duration = sum(seg['end'] - seg['start'] for seg in period)
        period_durations.append((period, duration))
    
    period_durations.sort(key=lambda x: x[1], reverse=True)
    
    # Try to find combination that fits target
    best_combination = []
    current_duration = 0
    
    for period, duration in period_durations:
        if current_duration + duration <= target_max:
            best_combination.extend(period)
            current_duration += duration
            
            if current_duration >= target_min:
                break
    
    # Return segments sorted by time
    if current_duration >= target_min:
        best_combination.sort(key=lambda x: x['start'])
        return best_combination
    
    return None

def create_combined_audio_sample(segments, audio_file_path, output_filename):
    """Create a single audio file from multiple segments."""
    if not segments:
        return
    
    # If only one segment, extract directly
    if len(segments) == 1:
        seg = segments[0]
        stream = ffmpeg.input(audio_file_path, ss=seg['start'], to=seg['end'])
        ffmpeg.output(stream, output_filename).run(overwrite_output=True, quiet=True)
        return
    
    # For multiple segments, concatenate them
    inputs = []
    for seg in segments:
        duration = seg['end'] - seg['start']
        segment_stream = ffmpeg.input(audio_file_path, ss=seg['start'], t=duration)
        inputs.append(segment_stream)
    
    # Concatenate all segments
    joined = ffmpeg.concat(*inputs, v=0, a=1)
    ffmpeg.output(joined, output_filename).run(overwrite_output=True, quiet=True)

if __name__ == "__main__":
    main()
