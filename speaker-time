#!/usr/bin/env python3

import sys
import os
import csv
import json
import ffmpeg
import whisper
from pyannote.audio import Pipeline

def main():
    if len(sys.argv) != 2:
        print("Usage: speaker-time <audio_file>")
        sys.exit(1)

    file_path = sys.argv[1]
    absolute_path = os.path.abspath(file_path)

    # Initialize pyannote pipeline
    hf_token = os.getenv("HUGGINGFACE_SPEAKER_DIARIZATION")
    if not hf_token:
        print("Error: HUGGINGFACE_SPEAKER_DIARIZATION environment variable not set")
        sys.exit(1)

    pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1", use_auth_token=hf_token)

    # Run speaker diarization
    diarization = pipeline(absolute_path)

    # Collect all segments for timeline processing
    segments = []
    for segment, _, speaker in diarization.itertracks(yield_label=True):
        segments.append({
            'speaker': speaker,
            'start': segment.start,
            'end': segment.end
        })

    # Sort segments by start time
    segments.sort(key=lambda x: x['start'])

    # Resolve overlapping segments (assign to original speaker)
    resolved_segments = []
    for segment in segments:
        # Check for overlaps with existing resolved segments
        overlapping = False
        for existing in resolved_segments:
            if (segment['start'] < existing['end'] and segment['end'] > existing['start']):
                # Overlap detected - keep the original speaker's segment
                overlapping = True
                break
        
        if not overlapping:
            resolved_segments.append(segment)

    # Generate output directory and CSV filename
    base_name = os.path.splitext(os.path.basename(file_path))[0]
    output_dir = os.path.join("output", base_name)
    os.makedirs(output_dir, exist_ok=True)
    csv_filename = os.path.join(output_dir, f"{base_name}_timeline.csv")

    # Write timeline to CSV
    with open(csv_filename, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['SPEAKER_ID', 'start_time', 'end_time'])
        
        for segment in resolved_segments:
            writer.writerow([segment['speaker'], segment['start'], segment['end']])

    print(f"Timeline saved to: {csv_filename}")
    print(f"Total segments: {len(resolved_segments)}")

    # Create audio subdirectory for segment files
    audio_dir = os.path.join(output_dir, "audio")
    os.makedirs(audio_dir, exist_ok=True)

    # Read CSV and cut audio segments
    print("Cutting audio segments...")
    with open(csv_filename, 'r') as csvfile:
        reader = csv.DictReader(csvfile)
        for i, row in enumerate(reader, 1):
            start_time = float(row['start_time'])
            end_time = float(row['end_time'])
            
            # Generate 6-digit zero-padded filename
            segment_filename = os.path.join(audio_dir, f"{i:06d}.mp3")
            
            # Cut audio segment using ffmpeg-python
            stream = ffmpeg.input(absolute_path, ss=start_time, to=end_time)
            ffmpeg.output(stream, segment_filename).run(overwrite_output=True, quiet=True)
    
    print(f"Audio segments saved to: {audio_dir}")
    print(f"Generated {len(resolved_segments)} audio files")

    # Generate language detection samples
    language_samples = generate_language_samples(resolved_segments, absolute_path, output_dir)
    
    # Detect languages for each speaker
    if language_samples:
        detect_speaker_languages(language_samples, output_dir)
        
        # Transcribe all audio segments
        transcribe_segments(output_dir)

def find_contiguous_segments(segments, gap_threshold=3.0):
    """Group segments by speaker and find contiguous periods within gap threshold."""
    speaker_groups = {}
    
    # Group segments by speaker
    for segment in segments:
        speaker = segment['speaker']
        if speaker not in speaker_groups:
            speaker_groups[speaker] = []
        speaker_groups[speaker].append(segment)
    
    # Find contiguous periods for each speaker
    contiguous_periods = {}
    for speaker, speaker_segments in speaker_groups.items():
        periods = []
        current_period = [speaker_segments[0]]
        
        for i in range(1, len(speaker_segments)):
            gap = speaker_segments[i]['start'] - current_period[-1]['end']
            
            if gap <= gap_threshold:
                # Continue current period
                current_period.append(speaker_segments[i])
            else:
                # Start new period
                periods.append(current_period)
                current_period = [speaker_segments[i]]
        
        # Add the last period
        periods.append(current_period)
        contiguous_periods[speaker] = periods
    
    return contiguous_periods

def generate_language_samples(segments, audio_file_path, output_dir):
    """Generate 20-60 second audio samples for language detection."""
    print("Generating language detection samples...")
    
    # Find contiguous segments
    contiguous_periods = find_contiguous_segments(segments)
    
    # Create language detection subdirectory
    lang_dir = os.path.join(output_dir, "language_detection")
    os.makedirs(lang_dir, exist_ok=True)
    
    generated_samples = {}
    
    for speaker, periods in contiguous_periods.items():
        # Calculate total speaking time for this speaker
        total_duration = sum(seg['end'] - seg['start'] for seg in segments if seg['speaker'] == speaker)
        
        # Skip speakers with less than 20 seconds total
        if total_duration < 20:
            print(f"Skipping {speaker}: only {total_duration:.1f}s total speaking time")
            continue
        
        # Find best combination of periods to get 20-60 seconds
        best_sample = find_best_sample_combination(periods, target_min=20, target_max=60)
        
        if best_sample:
            sample_filename = os.path.join(lang_dir, f"{speaker}_language_sample.mp3")
            create_combined_audio_sample(best_sample, audio_file_path, sample_filename)
            
            sample_duration = sum(seg['end'] - seg['start'] for seg in best_sample)
            print(f"Generated {speaker} language sample: {sample_duration:.1f}s -> {sample_filename}")
            generated_samples[speaker] = sample_filename
        else:
            print(f"Could not generate sufficient sample for {speaker}")
    
    return generated_samples

def find_best_sample_combination(periods, target_min=30, target_max=60):
    """Find the best combination of contiguous periods to reach target duration."""
    # Sort periods by total duration (largest first)
    period_durations = []
    for period in periods:
        duration = sum(seg['end'] - seg['start'] for seg in period)
        period_durations.append((period, duration))
    
    period_durations.sort(key=lambda x: x[1], reverse=True)
    
    # Try to find combination that fits target
    best_combination = []
    current_duration = 0
    
    for period, duration in period_durations:
        if current_duration + duration <= target_max:
            best_combination.extend(period)
            current_duration += duration
            
            if current_duration >= target_min:
                break
    
    # Return segments sorted by time
    if current_duration >= target_min:
        best_combination.sort(key=lambda x: x['start'])
        return best_combination
    
    return None

def create_combined_audio_sample(segments, audio_file_path, output_filename):
    """Create a single audio file from multiple segments."""
    if not segments:
        return
    
    # If only one segment, extract directly
    if len(segments) == 1:
        seg = segments[0]
        stream = ffmpeg.input(audio_file_path, ss=seg['start'], to=seg['end'])
        ffmpeg.output(stream, output_filename).run(overwrite_output=True, quiet=True)
        return
    
    # For multiple segments, concatenate them
    inputs = []
    for seg in segments:
        duration = seg['end'] - seg['start']
        segment_stream = ffmpeg.input(audio_file_path, ss=seg['start'], t=duration)
        inputs.append(segment_stream)
    
    # Concatenate all segments
    joined = ffmpeg.concat(*inputs, v=0, a=1)
    ffmpeg.output(joined, output_filename).run(overwrite_output=True, quiet=True)

def detect_speaker_languages(language_samples, output_dir):
    """Detect language for each speaker using Whisper and save to metadata.json."""
    print("Detecting languages with Whisper...")
    
    # Load Whisper model (using 'small' for good speed/accuracy balance)
    model = whisper.load_model("small")
    
    speaker_languages = {}
    
    for speaker, sample_file in language_samples.items():
        try:
            # Load and process audio for language detection
            audio = whisper.load_audio(sample_file)
            audio = whisper.pad_or_trim(audio)
            
            # Create mel spectrogram
            mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)
            
            # Detect language
            _, probs = model.detect_language(mel)
            detected_language = max(probs, key=probs.get)
            confidence = probs[detected_language]
            
            speaker_languages[speaker] = {
                "language": detected_language,
                "confidence": float(confidence)
            }
            
            print(f"{speaker}: {detected_language} (confidence: {confidence:.3f})")
            
        except Exception as e:
            print(f"Error detecting language for {speaker}: {e}")
            speaker_languages[speaker] = {
                "language": "unknown",
                "confidence": 0.0,
                "error": str(e)
            }
    
    # Save metadata to JSON file
    metadata_file = os.path.join(output_dir, "metadata.json")
    with open(metadata_file, 'w') as f:
        json.dump(speaker_languages, f, indent=2)
    
    print(f"Language metadata saved to: {metadata_file}")

def transcribe_segments(output_dir):
    """Transcribe all audio segments using Whisper and save to transcription.csv."""
    print("Transcribing audio segments...")
    
    # Load Whisper model
    model = whisper.load_model("small")
    
    # Load metadata for language hints
    metadata_file = os.path.join(output_dir, "metadata.json")
    speaker_languages = {}
    if os.path.exists(metadata_file):
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
            for speaker, data in metadata.items():
                speaker_languages[speaker] = data.get("language", "en")
    
    # Load timeline CSV to map segments to speakers and timestamps
    timeline_file = None
    for file in os.listdir(output_dir):
        if file.endswith("_timeline.csv"):
            timeline_file = os.path.join(output_dir, file)
            break
    
    if not timeline_file:
        print("Error: Timeline CSV file not found")
        return
    
    timeline_data = []
    with open(timeline_file, 'r') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            timeline_data.append(row)
    
    # Transcribe each audio segment
    audio_dir = os.path.join(output_dir, "audio")
    transcription_data = []
    
    for i, timeline_row in enumerate(timeline_data, 1):
        segment_file = os.path.join(audio_dir, f"{i:06d}.mp3")
        
        if not os.path.exists(segment_file):
            print(f"Warning: Audio segment {segment_file} not found")
            continue
        
        speaker_id = timeline_row['SPEAKER_ID']
        language = speaker_languages.get(speaker_id, "en")
        
        try:
            # Transcribe with language hint and timeout per segment
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("Transcription timeout")
            
            # Set timeout for individual segment (30 seconds per segment)
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(30)
            
            try:
                result = model.transcribe(segment_file, language=language)
                signal.alarm(0)  # Cancel timeout
                
                transcription_data.append({
                    'speaker_id': speaker_id,
                    'segment_id': i,
                    'start_time': timeline_row['start_time'],
                    'end_time': timeline_row['end_time'],
                    'text': result['text'].strip(),
                    'language': language,
                    'confidence': float(result.get('segments', [{}])[0].get('avg_logprob', 0.0)) if result.get('segments') else 0.0
                })
                
                print(f"Transcribed segment {i:06d} ({speaker_id}): {result['text'].strip()[:50]}...")
                
            except TimeoutError:
                signal.alarm(0)  # Cancel timeout
                print(f"Timeout transcribing segment {i:06d} (30s limit exceeded)")
                transcription_data.append({
                    'speaker_id': speaker_id,
                    'segment_id': i,
                    'start_time': timeline_row['start_time'],
                    'end_time': timeline_row['end_time'],
                    'text': "[TIMEOUT]",
                    'language': language,
                    'confidence': 0.0
                })
            
        except Exception as e:
            print(f"Error transcribing segment {i:06d}: {e}")
            transcription_data.append({
                'speaker_id': speaker_id,
                'segment_id': i,
                'start_time': timeline_row['start_time'],
                'end_time': timeline_row['end_time'],
                'text': "",
                'language': language,
                'confidence': 0.0
            })
    
    # Save transcription CSV
    transcription_file = os.path.join(output_dir, "transcription.csv")
    with open(transcription_file, 'w', newline='') as csvfile:
        fieldnames = ['speaker_id', 'segment_id', 'start_time', 'end_time', 'text', 'language', 'confidence']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(transcription_data)
    
    print(f"Transcription saved to: {transcription_file}")
    print(f"Transcribed {len(transcription_data)} segments")

if __name__ == "__main__":
    main()
